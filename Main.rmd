---
title: "R CheatSheet for ADS2 2020-21"
author: "YU Zhejian"
date: "23/10/2020"
output:
  word_document:
    toc: yes
    toc_depth: '5'
  pdf_document:
    toc: yes
    number_sections: yes
    toc_depth: 5
    keep_tex: yes
    extra_dependencies: ["times"]
    includes:
      in_header: headers.tex
  html_notebook:
    toc: yes
    toc_depth: 5
geometry: inner=0.5in,outer=0.5in
---

![YuZJLab_BG](YuZJLab_BG.png)

# Global Libraries

Locating global libraries.

```{r global libraries}
rm(list = ls()) # Remove cached variables
# Knitr Options
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 16)
knitr::opts_chunk$set(cache = TRUE)
sessionInfo()
version

# Source global packages
library(tidyverse)
library(ggpubr)
library(corrplot)
library(keras)
library(tensorflow)
library(torch)
library(C50)
library(pheatmap)
library(factoextra)
library(cluster)
library(stringr)
library(nnet)
library(lsr)
# Source global data
data("mpg")
data("women")
data("USArrests")
```

# Interesting things in R

TODO: Add parallel

## Replicate a function by `replicate()`

```{r replicate}
rm(list = ls())
c1 <- replicate(10, mean(sample(rnorm(100, 0, 100)), 20))
# Equals to:
c1 <- c()
for (i in c(1:10)) {
  c1[i] <- mean(sample(rnorm(100, 0, 100)), 20)
}
```

# Basic R Data Types

## Vector

### Generate or combine a vector by `c()` or `seq()`

```{r c}
rm(list = ls())
a <- seq(1, 10, by = 2) # Generate a vector.
a
b <- c(1, 10)
b
c <- c(a, b)
c # Link those vectors together.
a <- data.frame(1:5, 6:10)
c <- c(a, b)
c # This gives a list.
```

### Determine whether an item is inside a vector by `%in%`

```{r in}
1 %in% c(1, 2, 3)
c(1, 2) %in% c(1, 2, 3)
```

## Matrices

### Generate a matrix by `matrix()`

```{r matrix}
rm(list = ls())
matrix(data = c(9, 2, 3, 4, 5, 6), ncol = 3)
matrix(data = c(9, 2, 3, 4, 5, 6), ncol = 3, byrow = TRUE)
matrix(data = c(9, 2, 3, 4, 5, 6), nrow = 2, byrow = TRUE,
       dimnames = list(ROW_NAMES = c("a", "b"),
                       COL_NAMES = c("c", "d", "e")))
```

### Access an element by `name[row,col]`

```{r brackets}
rm(list = ls())
a <- matrix(data = c(9, 2, 3, 4, 5, 6), ncol = 3, byrow = TRUE)
a[1, 1]
a[1,]
a[, 1]
a[, c(1, 3)]
a[1]
# row or col can also be vector of logicals
a[c(TRUE, FALSE),]
a[a[, 1] > 5,]
```

## DataFrame

### Generate a dataframe by `data.frame()`

```{r data.frame}
rm(list = ls())
a <- data.frame(x1 = 1:5, x2 = 6:10)
a
```

### Access can be done by `name[row,col]` \& `name$col_name`

```{r dollar}
a$x1
```

### Get/Set column names by `names()`

```{r names}
rm(list = ls())
a <- c("l1", "l2") # a is a vector
names(a)
a <- data.frame(1:5, 6:10) # a is a dataframe
names(a)
names(a) <- c("l1", "l2")
names(a)
a <- list(one = 1, two = c(1, 2), five = seq(0, 1, length = 5))
names(a)
names(a) <- c("l1", "l2", "l3")
names(a)
```

There is a similar function named `dimnames()`, which allows you to change names in different dimensions. E.g. Column or row.


```{r dimnames}
rm(list = ls())
a <- matrix(data = c(9, 2, 3, 4, 5, 6),
            ncol = 3, byrow = TRUE,
            dimnames = list(ROW_NAMES = c("a", "b"),
                            COL_NAMES = c("c", "d", "e")))
dimnames(a)
dimnames(a) <- list(ROW_NAMES = c("1", "2"),
                    COL_NAMES = c("3", "4", "5"))
dimnames(a)
```

### Bind dataframes by `rbind()` \& `cbind()` \& `merge`

```{r rbind}
rm(list = ls())
a <- data.frame(1:5, 6:10)
b <- data.frame(11:15, 16:20)
c <- cbind(a, b)
c
# This line makes the column name of a and b identical
names(b) <- names(a)
d <- rbind(a, b)
d
a <- data.frame(idx = c(1:5), x = c(6:10), ab = c(11:15))
b <- data.frame(idx = c(1:5), y = c(16:20), ab = c(13:17))
merge(a, b, by = "idx")
merge(a, b, by = "ab")
```

## List

### Generate a list by `list()`

```{r list}
rm(list = ls())
list(x1 = c(1:5), x2 = data.frame(df1 = 1:5, df2 = 6:10))
```

### Access members by `name[[item_index]]` or `name$item_name`

Omitted.

## function

```{r function}
rm(list = ls())
dice <- function(max) { return(sample(c(1:max), 1, replace = FALSE)); }
dice(6)
```

## Conversions between datatype: `as.*` functions

TODO

## Determining datatype: `is.*`, `str()` or `class()` functions

TODO

For `str()` or `class()` functions, see below.

# Preparing to Work

## Cleanup

Remove all variables \& functions inside the memory and turn off all graphical devices.

```{r cleanup, eval=FALSE}
rm(list = ls())
dev.off()
```

## Working Directory

Set the working directory by `setwd()` and get the working directory by `getwd()`.

```{r setwd,eval = FALSE}
rm(list = ls())
setwd('~/Documents')
getwd()
```

## Package Management

### Package management by commands in `base`

#### Installation by `install.packages()`

```{r install.packages,eval=FALSE}
rm(list = ls())
install.packages("devtools")
# Please refer to file devtools.log to see full output.
```

#### Upgrade by `update.packages()`

```{r update.packages ,eval=FALSE}
rm(list = ls())
update.packages(ask = FALSE)
```

#### Loading a package by `library()`

```{r library,eval=FALSE}
# Load tidyverse
library(tidyverse)
```

### Package Management by `devtools` & `git2r`

`install_*()` reinstalls the package, detaches the currently loaded version then reloads the new version with `library()`.

`build()` builds a package file from package sources. You can use it to build a binary version of your package.

```{r devtools,eval=FALSE}
# Firstly we clone dplyr into our local folder `dplyr`.
git2r::clone("https://github.com/tidyverse/dplyr/", "dplyr")
devtools::install_local("dplyr") # Install dplyr
# Equals to:
devtools::install_github("tidyverse/dplyr")
# Equals to:
devtools::install_git("https://github.com/tidyverse/dplyr/")
# Please referr to file dplyr.log to see full output.
```

You may also install packages by `install_bitbucket()` & `install_github()` & `install_gitlab()` & `install_git()` & `install_svn()` & `install_url()`.

`update_packages()` updates a package to the latest version. This works both on packages installed from CRAN as well as those installed from any of the `install_*` functions. **NOTICE: Build on source by default.**

### Package Management by `BioConductor`

`Bioconductor` <http://www.bioconductor.org/> provides tools for the analysis and comprehension of high-throughput genomic data. Bioconductor uses the R statistical programming language, and is open source and open development. It has two releases each year, and an active user community.

e.g. install `biomaRt` by `BioConductor`:

```{r biocmanager,eval=FALSE}
install.packages("BiocManager")
BiocManager::install("biomaRt")
# Please referr to file biomart.log to see full output.
# Update:
BiocManager::install(update = TRUE, ask = FALSE)
```

# File Read/Write

## File download

### File download by `downloader` package

```{r downloader,eval=FALSE}
# Download an R file:
# (We may use http://sf.R as filename)
downloader::download("http://sf.R")
# Source an R file:
downloader::source_url("http://sf.R")
```

### File download by `download.file()`

```{text}
download.file(url, destfile, method, quiet = FALSE, mode = "w")

Args:
    url <- character. The URL to download.
    destfile <- character. Where the downloaded file is saved.
    method <- character. Downloading method to use.
        It actually calls `download.file()`.
        Avail: libcurl (Default), wget, curl ...
        ERR: If external libcurl corrupts, you will get segmental fault.
    quite <- logical. Whether to supress output.
    mode <- character. The mode with which to write the file. Useful
    values are "w", "wb" (binary), "a" (append) and "ab". Not used for
    methods "wget" and "curl". See also ‘Details’, notably about using
    "wb" for Windows.
```

## Write data by `write.*` functions

```{text}
write.table(x, file = "", append = FALSE, quote = TRUE, sep = " ", eol = "\n",
na = "NA", dec = ".", row.names = TRUE, col.names = TRUE, fileEncoding = "")

Args:
    x <- dataframe/matrix to write.
    file <- character. File to read.
    append <- logical. Add at bottom or overwrite the file.
    qutoe <- logical. Field may wrapped by quotation marks.
    sep <- character. Field separator.
    eol <- character. End-of-line character.
    na <- character. How to write NA.
    dec <- character. Demical sign.
    row.names <- logical. Write row names or not.
    row.names <- vector. What to erite as rwo names.
    col.names <- referr2 row.names.
```

```{r write}
rm(list = ls())
write.table(mpg, file = "mpg.tsv", sep = "\t")
```

## Read data by `read.*` functions

```{text}
read.table(file, header = FALSE, sep = "", quote = "\"'", dec = ".",
blank.lines.skip = TRUE, na.strings = "NA", comment.char = "#",
allowEscapes = FALSE, fileEncoding = "", encoding = "unknown",
stringsAsFactors = default.stringsAsFactors())

read.csv <- read.table(file, header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "", ...)

read.csv2 <- read.table(file, header = TRUE, sep = ";", quote = "\"",
dec = ",", fill = TRUE, comment.char = "", ...)

read.delim <- read.table(file, header = TRUE, sep = "\t", quote = "\"",
dec = ".", fill = TRUE, comment.char = "", ...)

read.delim2 <- read.table(file, header = TRUE, sep = "\t", quote = "\"",
dec = ",", fill = TRUE, comment.char = "", ...)

Args:
    file <- referr2 write.tables
    header <- logical. Whether to read 1st line as header (column name).
    sep <- referr2 write.tables
    qutoe <- referr2 write.tables
    dec <- referr2 write.tables
    blank.lines.skip <- logical. Skip blank lines.
    na.strings <- character. Strings for NA.
    comment.char <- charater. Skip lines leading by this.
    allowEscapes <- logical. Allow escaping character.
    fileEncoding <- referr2 write.tables
    encoding <- ?
    stringsAsFactors <- logical. Convert strings to factors.

Return/Accept: dataframe <- read.table
```

```{r read}
rm(list = ls())
a <- read.delim("mpg.tsv")
head(a)
unlink("mpg.tsv") # Delete this file
```

## Package `readr` (FAILED)

As a sub-package of `tidyverse`, `readr` provides a fast and friendly way to read rectangular data. Compared to the corresponding base functions, readr functions are said to be faster and more user-friendly.

Importing the library: `tidyverse` to import `tidyr`, `dplyr`, `readr` and `gglot2`, `readr` to import readr only


```{text}
read_delim(file, delim, quote = "\"", escape_backslash = FALSE,
escape_double = TRUE, col_names = TRUE, col_types = NULL,
locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
comment = "", trim_ws = FALSE, skip = 0, n_max = Inf,
guess_max = min(1000, n_max), progress = show_progress(),
skip_empty_rows = TRUE)

read_csv(file, col_names = TRUE, col_types = NULL,
locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
quote = "\"", comment = "", trim_ws = TRUE, skip = 0, n_max = Inf,
guess_max = min(1000, n_max), progress = show_progress(),
skip_empty_rows = TRUE)

read_csv2(file, col_names = TRUE, col_types = NULL,
locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
quote = "\"", comment = "", trim_ws = TRUE, skip = 0, n_max = Inf,
guess_max = min(1000, n_max), progress = show_progress(),
skip_empty_rows = TRUE)

read_tsv(file, col_names = TRUE, col_types = NULL,
locale = default_locale(), na = c("", "NA"), quoted_na = TRUE,
quote = "\"", comment = "", trim_ws = TRUE, skip = 0, n_max = Inf,
guess_max = min(1000, n_max), progress = show_progress(),
skip_empty_rows = TRUE)

Args:
    file <- character/vector of character.
         Either a path to a file, a connection, or literal data
        (either a single string or a raw vector).
        Files ending in .gz, .bz2, .xz, or .zip will be automatically
        uncompressed. Files starting with http://, https://, ftp://,
        or ftps:// will be automatically downloaded. Remote gz files
        can also be automatically downloaded and decompressed.
    delim <- referr2 read.table
    quote <- referr2 read.table
    escape_backslash <- referr2 read.table
    escape_double <- referr2 read.table
    col_names <- logical/vector of character. Read/customize headers.
    col_types <- NULL/cols()/character. Omitted.
    locale <- Omitted.
    na <- vector of character. Strings for NA.
    quoted_na <- logical. Should missing values inside quotes be treated
        as missing values (the default) or strings.
    comment <- referr2 read.table
    trim_ws <- logical. Should leading and trailing whitespace be trimmed
        from each field before parsing it?
    skip <- integral. Number of lines to skip before reading data.
    n_max <- integral. Maximun number of lines to read.
    progress <- logical. Show progressbar?
    skip_empty_rows <- logical. Ignore blank lines?
```


# Describing data

## Getting the first/last n rows and header by `head()`/`tail()`

```{r head tail}
rm(list = ls())
head(mpg)
head(mpg, n = 3)
tail(mpg)
tail(mpg, n = 3)
```

## Getting length and width of an object by `length()` \& `dim()` \& `nrow()` \& `ncol()`

```{r length dim nrow ncol}
rm(list = ls())
b <- c(1:5)
length(b) # How long a vector is.
dim(b)
nrow(b)
ncol(b)
b <- data.frame(x1 = 1:5, x2 = 6:10)
length(b) # How wide (column number) a dataframe is.
length(b$x1) # How long (row number) a dataframe is.
dim(b) # Length (row number), width (column number)
nrow(b)
ncol(b)
rm(b)
```

## Select data by `which()` \& `which.*()`

```{r which}
rm(list = ls())
b <- mpg$hwy >= 24
head(b) # Cars with highway miles per gallon >=24
c <- which(b)
c
mpg[c,]
# equals to
mpg[which(mpg$hwy >= 24),]
# or
mpg[mpg$hwy >= 24,]
which.max(c(1:5))
which.min(c(1:5))
```

## Get mean/sd/quarters of each column by `summary()`

```{r summary}
rm(list = ls())
summary(c(1:5))
summary(mpg)
```

## Get statistics of vector by `mean()` \& `meian()` \& `sd()` \& `sum()`

Omitted

Have to use numeric/logical vectors, otherwise `NA` will be returned. Omitted.

## Get structure of R objects by `str()`

```{r str}
rm(list = ls())
b <- c(1:5)
str(b)
b <- data.frame(x1 = 1:5, x2 = 6:10)
str(b)
str(mpg)
```

## Get the data type of object by `class()`

```{r class}
rm(list = ls())
class(c(1:5))
class(matrix(data = c(9, 2, 3, 4, 5, 6), ncol = 3))
class(data.frame(x1 = 1:5, x2 = 6:10))
```

## Get/Set object attribute lists by `attributes()`

```{r attributes}
rm(list = ls())
date1 <- as.POSIXct("2020-01-07", tz = "GMT")
attributes(date1)
date1
attributes(date1)$tzone <- "America/New_York"
attributes(date1)
date1
```

## Determine/Eliminate the existence of NA by `anyNA()` \& `complete.cases()` \& `na.omit()` \& `tidyr::drop_na()`

```{r renove NA}
rm(list = ls())
a <- data.frame(x1 = c(1, 2, 3, NA, 5), x2 = 6:10)
anna <- data.frame(x1 = c(1:5), x2 = 6:10)
m <- matrix(data = c(9, 2, NA, 4, 5, 6), ncol = 3)
mnna <- matrix(data = c(9, 2, 3, 4, 5, 6), ncol = 3)
anyNA(c(1:5))
anyNA(c(1, 2, NA, 4, 5))
anyNA(mnna)
anyNA(m)
anyNA(anna)
anyNA(a)
complete.cases(c(1:5))
complete.cases(c(1, 2, NA, 4, 5))
complete.cases(mnna)
complete.cases(m)
complete.cases(anna)
complete.cases(a)
a[complete.cases(a),]
na.omit(a)
na.exclude(a) # TODO
tidyr::drop_na(a)
apply(is.na(a), 2, which)
```

## Determine/Eliminate the existence of dulicates by `duplicated()` and `unique()`

```{r duplicated unique}
rm(list = ls())
head(duplicated(mpg))
which(duplicated(mpg))
sum(duplicated(mpg))
dim(mpg)
dim(mpg[which(!duplicated(mpg)),]) # SLOW!
dim(unique(mpg))
```

# Generating new data from existing data \& *de novo*

## Import embedded datasets by `data()`

Function `data()` can import those datasets embedded in packages, or listing all available datasets if no arguments provided.

```{r data}
data(mpg)
# datasets in ggplot2
head(data(package = "ggplot2")$results, n = 3)
# datasets available now
head(data()$results, n = 3)
# data(package = .packages(all.available = TRUE))
# list the data sets in all *available* packages.
```

## Random sampling by `sample()`

```{r sample}
rm(list = ls())
a <- c(1:5)
sample(a, 1)
sample(a, 10, replace = TRUE)
sample(a, 10, replace = TRUE,
       prob = c(0.1, 0.2, 0.3, 0.4, 0.0))
mpg[sample(seq_along(mpg$hwy), 5),]
# seq_along is a safer version of c(1:...)

# Sample skills
# To reorder records randomly
order(runif(c(1:10)))
```

## *de novo* generation of data by  `stats`

### Normal distribution by `rnorm()`

Normal Distribution is a bell-shaped frequency distribution curve which helps describe all the possible values a random variable can take within a given range with most of the distribution area is in the middle and few are in the tails, at the extremes.

```{r rnorm}
rm(list = ls())
a <- rnorm(2000, mean = 100, sd = 7)
x <- seq(-10, 10, 0.1)
gghistogram(a)
```

Other functions for normal distributions:

Use `pnorm(val)` to calculate the Cumulative Distribution Function value for `val`.

Use `qnorm(val)` as an inverse operation.

Use `dnorm(val)` to get the Probability Density Function value for `val` in a distribution.

e.g.

```{r pnorm dnorm}
x <- seq(-10, 10, 0.1)
a <- data.frame(x = x, y = pnorm(x))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for Normal Distribution")
a <- data.frame(x = x, y = dnorm(x))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for Normal Distribution")
```

Normal distributions can also be checked by *Normal Q-Q Plot* or *Shapiro-Wilk test*

### Exponential Distribution by `rexp()`

```{r rexp}
x <- seq(-2, 5, 0.05)
a <- rexp(n = 2000, rate = 2)
gghistogram(a)
a <- data.frame(x = x, y = pexp(x))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for Exponential Distribution")
a <- data.frame(x = x, y = dexp(x))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for Exponential Distribution")
```

### Uniform Distribution by `runif()`

Please note that it should be regarded as "r unif" instead of "run if".

```{r runif}
x <- seq(-10, 10, 0.1)
a <- runif(n = 2000, min = 0, max = 100)
gghistogram(a)
a <- data.frame(x = x, y = punif(x))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for Uniform Distribution")
a <- data.frame(x = x, y = dunif(x))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for Uniform Distribution")
```

### Student's T Distribution by `rt()`

A statistical distribution published by William Gosset in 1908. His employer, Guinness Breweries, required him to publish under a pseudonym, so he chose "Student". Given $N$ independent measurements $x_{i}$, let

$$
t=\frac{\bar{x}-\mu}{s/\sqrt{N}}
$$

where $\mu$ is the population mean, $\bar{x}$ is the sample mean, and $s$ is the estimator for population standard deviation.

```{r rt}
x <- seq(-10, 10, 0.1)
a <- rt(n = 2000, df = 2)
gghistogram(a)
# Dots = normal distribution
# black, red, green: df = 1, 2, 3
a <- data.frame(x = x,
                y1 = pt(x, 1),
                y2 = pt(x, 2),
                y3 = pt(x, 3),
                yn = pt(x, 4))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = yn)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for Student's T Distribution")
a <- data.frame(x = x,
                y1 = dt(x, 1),
                y2 = dt(x, 2),
                y3 = dt(x, 3),
                yn = dt(x, 4))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = yn)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for Student's T Distribution")
```

### F distribution by `rf()`

If U and V are independent chi-square random variables with $r_{1}$ and $r_{2}$ degrees of freedom, respectively, then:

$$
F=\frac{U/r_{1}}{V/r_{2}}
$$

follows an F-distribution with $r_{1}$ numerator degrees of freedom and $r_{2}$ denominator degrees of freedom. We write $F ~ F(r_{1},r_{2})$.

```{r rf}
x <- seq(-2, 5, 0.05)
# Dots = normal distribution
# black, red, green: df = 1, 2, 3
a <- data.frame(x = x,
                y1 = pf(x, df1 = 1, df2 = 81),
                y2 = pf(x, df1 = 81, df2 = 1),
                y3 = pf(x, df1 = 1, df2 = 1),
                y4 = pf(x, df1 = 81, df2 = 81))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = y4)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for F Distribution")
a <- data.frame(x = x,
                y1 = df(x, 1, 81),
                y2 = df(x, 81, 1),
                y3 = df(x, 1, 1),
                y4 = df(x, 81, 81))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = y4)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for F Distribution")
a <- data.frame(x = x,
                y1 = pf(x, Inf, 1),
                y2 = pf(x, 1, Inf),
                y3 = pf(x, 1, 1),
                y4 = pf(x, Inf, Inf))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = y4)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for F Distribution")
a <- data.frame(x = x,
                y1 = df(x, Inf, 1),
                y2 = df(x, 1, Inf),
                y3 = df(x, 1, 1),
                y4 = df(x, Inf, Inf))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = y4)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for F Distribution")
```

### Poisson Distribution by `rpois()`

```{r rpois}
x <- seq(0, 25, 1)
a <- rpois(2000, lambda = 10)
gghistogram(a)
# Dots = normal distribution
# black, red, green: lambda = 1, 5, 15
a <- data.frame(x = x,
                y1 = ppois(x, lambda = 1),
                y2 = ppois(x, lambda = 5),
                y3 = ppois(x, lambda = 15),
                yn = pnorm(x, mean = 0))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = yn)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for Poisson Distribution")
a <- data.frame(x = x,
                y1 = dpois(x, 1),
                y2 = dpois(x, 5),
                y3 = dpois(x, 15),
                yn = dnorm(x, 0))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = yn)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for Poisson Distribution")
```

### Chi-Square Distribution by `rchisq()`

```{r rchisq}
x <- seq(0, 25, 1)
a <- rchisq(2000, df = 10)
gghistogram(a)
# Dots = normal distribution
# black, red, green: df = 1, 5, 15
x <- seq(0, 25, 1)
a <- data.frame(x = x,
                y1 = pchisq(x, df = 1),
                y2 = pchisq(x, df = 5),
                y3 = pchisq(x, df = 15),
                yn = pnorm(x, mean = 0))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = yn)) +
        labs(x = "X", y = "CDF(X)",
             title = "Cumulated Distribution Function for Chi-Square Distribution")
a <- data.frame(x = x,
                y1 = dchisq(x, 1),
                y2 = dchisq(x, 5),
                y3 = dchisq(x, 15),
                yn = dnorm(x, 0))
ggplot(a, aes(x = x)) +
        geom_line(aes(y = y1)) +
        geom_line(aes(y = y2), color = "red") +
        geom_line(aes(y = y3), color = "blue") +
        geom_point(aes(y = yn)) +
        labs(x = "X", y = "PDF(X)",
             title = "Probability Distribution Function for Chi-Square Distribution")
```

# Performing statistical tests by `stats`

## Test to Compare Two Means

### Students'/Welch's T-test by `t.test()`

Can test:

* Whether the mean of a sample is equal to predicted mean
* Whether the mean of two different group is the same

Prerequisites:

* Normal
* IID (if not paired)
* Homogeneity of variance (if not Welch)

```{r t.test}
t.test(c(1, 2, 3, 4, 5, 5, 6, 7, 8, 9), mu = 1) # One sample
t.test(c(1, 2, 3, 4, 5, 5, 6, 7, 8, 9), c(1, 2, 3)) # Welch Two Sample t-test
t.test(c(1, 2, 3, 4, 5, 5, 6, 7, 8, 9), c(1, 2, 3), var.equal = TRUE) # Students' t-test
t.test(c(1, 2, 3, 4, 5, 5, 6, 7, 8, 9), c(1, 2, 3), alternative = "greater")
t.test(c(1, 2, 3, 4, 5, 5, 6, 7, 8, 9), c(1, 2, 3), alternative = "less")
head(mpg, n = 10)
# Perform t-test using 2 columns of one dataset
t.test(x = mpg$cty, y = mpg$hwy)
# Perform t-test using groups
t.test(data = mpg[1:10,], cty ~ cyl)
```

The power of T-test can be checked by `power.t.test()`.

### Wilcoxon Rank Sum Test by `wilcox.test()`

Syntax similar to `t.test()`, but can be used when the sample is not that normal.

### F test by `var.test()`

Syntax similar to `t.test()`. It checks whether there's a difference in **VARIANCE**.

## Test to Compare Several Means

### One-way ANOVA (Analyse of Variance) using `aov()`

To assess more than one aspect of one factor's impact.

e.g. effect of 3 doses of a drug on heart rate

Assumptions:

* Independent random sampling
* Normality of residuals
* Equality of Variances

```{r aov}
rm(list = ls())
diseases <- data.frame(x = c(2.9, 3.0, 2.5,
                             2.6, 3.2, 3.8,
                             2.7, 4.0, 2.4,
                             2.8, 3.4, 3.7,
                             2.2, 2.0),
                       g = factor(rep(1:3, c(5, 4, 5)),
                                  labels = c("Normal subjects",
                                             "Subjects with obstructive airway disease",
                                             "Subjects with asbestosis")))
disease_aov <- aov(x ~ g, diseases)
summary(disease_aov)
coefficients(disease_aov)
shapiro.test(resid(disease_aov)) # Normality
bartlett.test(x ~ g, diseases)
plot(disease_aov, 1) # Equality of variance
TukeyHSD(disease_aov) # Tukey’s HSD (Honestly Significant Difference) test
```

### One-way Welch Test by `oneway.test()`

Syntax similar to `aov()`, but can be used when the sample's variance is different.

```{r oneway.test}
rm(list = ls())
diseases <- data.frame(x = c(2.9, 3.0, 2.5,
                             2.6, 3.2, 3.8,
                             2.7, 4.0, 2.4,
                             2.8, 3.4, 3.7,
                             2.2, 2.0),
                       g = factor(rep(1:3, c(5, 4, 5)),
                                  labels = c("Normal subjects",
                                             "Subjects with obstructive airway disease",
                                             "Subjects with asbestosis")))
oneway.test(x ~ g, diseases)
```

### Kruskal-Wallis Rank Sum Test by `kruskal.test()`

Syntax similar to `aov()`, but can be used when the sample is not that normal.

```{r kruskal.test}
rm(list = ls())
diseases <- data.frame(x = c(2.9, 3.0, 2.5,
                             2.6, 3.2, 3.8,
                             2.7, 4.0, 2.4,
                             2.8, 3.4, 3.7,
                             2.2, 2.0),
                       g = factor(rep(1:3, c(5, 4, 5)),
                                  labels = c("Normal subjects",
                                             "Subjects with obstructive airway disease",
                                             "Subjects with asbestosis")))
kruskal.test(x ~ g, diseases)
```

### n-way (Factorial) ANOVA using `aov()`

e.g. To assess the effect of block, N, P, K on Peas

```{r multi-aov}
rm(list = ls())
aggregate(yield ~ N * P * K, npk, mean) # To sum the data using N, P, K only
npk_aov <- aov(yield ~ block + N * P * K, npk)
summary(npk_aov)
coefficients(npk_aov)
shapiro.test(resid(npk_aov)) # Normality
plot(npk_aov, 1) # Equality of variance
TukeyHSD(npk_aov)
```

## Test to Distinguish Variance Equality

### Bartlett Test of Homogeneity of Variances Using `bartlett.test()`

```{r bartlett.test}
rm(list = ls())
diseases <- data.frame(x = c(2.9, 3.0, 2.5,
                             2.6, 3.2, 3.8,
                             2.7, 4.0, 2.4,
                             2.8, 3.4, 3.7,
                             2.2, 2.0),
                       g = factor(rep(1:3, c(5, 4, 5)),
                                  labels = c("Normal subjects",
                                             "Subjects with obstructive airway disease",
                                             "Subjects with asbestosis")))
bartlett.test(x ~ g, diseases)
```

## Test to Distinguish whether a Sample Obeys Normal Distribution

### Shapiro-Wilk Test by `shapiro.test()`

To test whether a dataset `x` is normally distributed.

```{r shapiro.test}
rm(list = ls())
shapiro.test(c(1, 1, 1, 1, 1, 1, 1, 1, 2))
hist(c(1, 1, 1, 1, 1, 1, 1, 1, 2))
shapiro.test(rnorm(1000, 1, 2))
hist(rnorm(1000, 1, 2))
```

### Normal Q-Q Plot by `qqnorm()`

```{r qqnorm}
rm(list = ls())
norm_sample <- sample(rnorm(n = 1000, mean = 10, sd = 1), 400)
hist(norm_sample)
qqnorm(norm_sample)

shapiro.test(norm_sample)
rm(norm_sample)
```

## Test whether a/some Factor(s) Impact the Data

Definition of Categorical Data:

* Values only chosen from discrete and finite values (categories)
* Values are mutually exclusive. Each subject can only choose one category
* Nominal and ordinal
* Nominal variables have no particular order: Hair color, race, gender…
* Ordinal variables have some inherent ordering: Pain levels, ratings…

### `chisq.test()` for Big Datasets

#### An 1-Way Example (Goodness-of-Fit)

We compare our data to a "model" (many times these models have percentages and we need to remember to convert these percentages to "counts"),and only one population.

e. g. For a matrix like

|      | C1   | Predict |
| ---- | ---- | ------- |
| R1   | a    | a'      |
| R2   | c    | b'      |

$$
\chi^2=\sum(\frac{(O-E)^2}{E})=\sum_{i=\{a,b\}}\frac{(i-i')^2}{i'}
$$

And then you may find

$$
p=1-PDF(\chi^2)
$$

Define degree of freedom ($df$):

df=number of cells - # of pieces of sample information required for computing expected cell frequencies.

In 1-way example,

$$
df=c-1
$$

```{r 1way chisq}
rm(list = ls())
mat_test <- matrix(data = c(0.1, 0.41, 0.49, 0.1, 0.4, 0.5),
                   byrow = TRUE, nrow = 2)
dimnames(mat_test) <- list(Group = c("Actual", "Except"),
                           Status = c("Good", "Fair", "Bad"))
mat_test
chisq.test(mat_test)
# This matrix should be tested by
chisq.test(x = c(0.1, 0.41, 0.49), p = c(0.1, 0.4, 0.5))
```

#### An 2-Way Example

Test for Homogeneity: Compare two or more different populations, one categorical variable

Test for Independence/Association: We have one population and want to compare two or more variables

e. g. For a matrix like

|      | C1   | C2   |
| ---- | ---- | ---- |
| R1   | a    | b    |
| R2   | c    | d    |

We wish to test whether the distribution of R1 and R2 in C1 and C2 is the same

Firstly we calculate the expectations.

|      | C1   | C2   | Sum     |
| ---- | ---- | ---- | ------- |
| R1   | a    | b    | a+b     |
| R2   | c    | d    | c+d     |
| Sum  | a+c  | b+d  | a+b+c+d |

|      | Expectations of C1      | Expectations of C2      | Sum     |
| ---- | ----------------------- | ----------------------- | ------- |
| R1   | a'=(a+c)(a+b)/(a+b+c+d) | b'=(b+d)(a+b)/(a+b+c+d) | a+b     |
| R2   | c'=(a+c)(c+d)/(a+b+c+d) | d'=(b+d)(c+d)/(a+b+c+d) | c+d     |
| Sum  | a+c                     | b+d                     | a+b+c+d |

So,

$$
\chi^2=\sum(\frac{(O-E)^2}{E})=\sum_{i=\{a,b,c,d\}}\frac{(i-i')^2}{i'}
$$

```{r 2way chisq}
rm(list = ls())
mat_test <- matrix(data = c(0.1, 0.41, 0.49, 0.1, 0.4, 0.5),
                   byrow = TRUE, nrow = 2)
dimnames(mat_test) <- list(Group = c("Group A", "Group B"),
                           Status = c("Good", "Fair", "Bad"))
mat_test
mat_chisq <- chisq.test(mat_test)
mat_chisq$observed
mat_chisq$expected
```

in a 2-way example,

$$
df=(r-1)(c-1)
$$

#### Examples in more categories (3-way):

| Genotype | Sex  | Alive | Dead |
| -------- | ---- | ----- | ---- |
| WT       | M    | a     | b    |
| WT       | F    | c     | d    |
| KO       | M    | e     | f    |
| KO       | F    | g     | h    |

For example,

$$
\bar{a}=\sum*\frac{\sum_{WT}}{\sum}\frac{\sum_{M}}{\sum}\frac{\sum_{Alive}}{\sum}
$$

How to test in R TODO.

### `fisher.test()` for Small Datasets

e. g. For a matrix like

|      | C1   | C2   |
| ---- | ---- | ---- |
| R1   | a    | b    |
| R2   | c    | d    |

Its p-value may be calculated as
$$
p=\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!(a+b+c+d)!}
$$
It can also be applied to bigger datasets.
$$
p=\frac{\prod(\sum \text{of each column \& row})}{\prod \text{of each cell and its sum}}
$$
Syntax similar to `chisq.test()`, but can be used when the sample size is small.

```{r fisher.test}
rm(list = ls())
mat_test <- matrix(data = c(10, 41, 49, 10, 40, 50),
                   byrow = TRUE, nrow = 2)
dimnames(mat_test) <- list(Group = c("Group A", "Group B"),
                           Status = c("Good", "Fair", "Bad"))
mat_test
fisher.test(mat_test)
```

### `mcnemar.test()` for Marginal Homogeneity

e. g. To test whether the answer to a question (Y/N) have changed after teaching

|         | Before:Y | Before:N |
| ------- | -------- | -------- |
| After:Y | 5        | 1        |
| After:N | 5        | 9        |

```{r mcnemar.test}
rm(list = ls())
mat_test <- matrix(data = c(5, 5, 1, 9), byrow = TRUE, nrow = 2)
dimnames(mat_test) <- list(Before = c("Y", "N"), After = c("Y", "N"))
mat_test
mcnemar.test(mat_test)
```

## Test whether the Dataset can be Represented in a Linear Form

### Correlation Testing Using `cor.test()`

```{r cor.test}
cor.test(women$height, women$weight)
x <- c(1:50)
cor.test(x, 2 * x + 1)
cor.test(x, x^2)
cor.test(x, runif(50, 0, 50))
```

### Linear Regression Using `lm()`

Prerequisites:

* Normality and independency of the residuals
* Linearity
* Homogeneity of variance
* Uncorrelated predictors
* No significant outliers

#### 1-factor (Simple) Linear Regression

Lines are fixed by minimizing sum of the *residuals*, which is

$$
\sum(Y-\hat{Y})^{2}
$$

where

$$
\hat{Y}=kX+b
$$

We define *sample correlation coefficient* (`r`) by

$$
r=\frac{Cov(x,y)}{\sqrt{(S_{x}^{2}(S_{y}^{2}))}}=\frac{\frac{\sum(Y-\hat{Y})(X-\hat{X})}{n-1}}{(\frac{\sum(X-\hat{X})^{2}}{n-1})(\frac{\sum(Y-\hat{Y})^{2}}{n-1})}=(n-1)\frac{\sum(Y-\hat{Y})(X-\hat{X})}{(\sum(X-\hat{X})^{2})(\sum(Y-\hat{Y})^{2})}
$$

with `r` ranges from -1 to 1. Thus,

$$
k=r\frac{S_{y}}{S_{x}}=r\frac{\sum(Y-\hat{Y})^{2}}{\sum(X-\hat{X})^{2}}
$$

$$
b=\hat{Y}-k\hat{X}
$$

Residual Standard Error (RSE, Goodness of Fit)

$$
RSE=\sqrt{\frac{\sum(Y-\hat{Y})^{2}}{\sum(Y-\bar{Y})^{2}}}
$$

$R^{2}$, or the coefficient of determinator

TODO: Errors here

$$
R^{2}=1-RSE^2
$$

means that the predictor explains $R^{2}$ of the variance in the outcome.

```{r lm}
rm(list = ls())
lm_women <- lm(weight ~ height, data = women)
cor(women$height, women$weight)
predict(lm_women,
        data.frame(height = c(57, 75, 88)))
summary(lm_women)
# Multiple R-squared: R^{2}
# Adjusted...
ggscatter(women, x = "height", y = "weight",
          add = "reg.line",
          add.params = list(color = "blue", fill = "lightgray"),
          conf.int = TRUE)
```

#### n-factor Linear Regression

Take 2 way for example. The formular will become:

$$
y=k_{1}x_{1}+k_{2}x_{2}+b
$$

```{r}
rm(list = ls())
lm_iris <- lm(formula = Sepal.Length ~ Sepal.Width + Species, data = iris)
```

TODO: Multiple factors

# Performing statistical tests *de novo*

## Bootstrap

### Case Resampling: Generating a Confidence Interval from Limited Number of Data

e.g. Generate confidence interval of mean from samples of unknown distribution

1. Randomly select LENGTH observations from the sample, with replacement after each selection. Some observations may be selected more than once, and some may not be selected at all.
2. Calculate and record the sample mean.
3. Repeat the first two steps 1,000 times.
4. Order the 1,000 sample means from smallest to largest.
5. Find the sample means representing the 2.5th and 97.5th percentiles. In this case, it’s the 25th number from the bottom and top. These are your 95% confidence limits.

```{r Case Resampling}
my_bootstrap <- function(x) {
  my_means <- replicate(1000, {
    mean(sample(x, length(x), replace = TRUE))
  })
  hist(my_means)
  return(list(mean = mean(my_means),
              up = quantile(my_means, probs = c(0.975)),
              low = quantile(my_means, probs = c(0.025))))
}
my_bootstrap(c(1:10))
```

e.g. Predict confidence interval of incremental levels from incremental levels

```{r Case Resampling 2}
covid <- data.frame(matrix(data = c(80796, 18, 12462, 3497, 10075, 1075),
                           nrow = 3, byrow = TRUE))
names(covid) <- c("Total", "New")
covid$Country <- c("China", "Italy", "Iran")
covid$Old <- covid$Total - covid$New
c <- data.frame(x = c(), y = c(), ymin = c(), ymax = c())
for (Country in unique(covid$Country)) {
  cc <- covid[covid$Country == Country, c("Old", "New", "Total")]
  ccb_n <- my_bootstrap(c(replicate(cc$Old, 0),
                          replicate(cc$New, 1)))
  c <- rbind(c, data.frame(x = Country,
                           y = ccb_n$mean,
                           ymin = ccb_n$low,
                           ymax = ccb_n$up))
}
ggplot(data = c, aes(x = reorder(x, y), y = y)) +
        geom_point() +
        geom_errorbar(aes(ymin = ymin, ymax = ymax))
```

### Permutation Test: Whether a Factor Influences the Data

This can be used to test whether a factor influences the data by comparing different statistics.

Use t-test if the statistics is mean.

* Sample (without replacement)
* Sample size is size of original dataset
* This essentially amounts to re-distributing labels
* Compare between new samples with different labels
* Repeat many times

e.g. Assess whether there's a difference in median of two groups.

```{r Permutation Test}
rm(list = ls())
data <- data.frame(grp1 = c(1, 2, 3, 4, 5), grp2 = c(2, 3, 4, 5, 6))
t.test(data$grp1, data$grp2)
# Now we wish to test MEDIAN
meian_bootstrap <- function(x, y) {
  merged <- c(x, y)
  xlen <- length(x)
  ylen <- length(y)
  allen <- xlen + ylen
  values <- replicate(1000, {
    neworder <- merged[order(runif(c(1:allen)))]
    newx <- neworder[1:xlen]
    newy <- neworder[xlen + 1:ylen]
    median(newx) - median(newy)
  })
  hist(values)
  return(list(mean = mean(values),
              up = quantile(values, probs = c(0.975)),
              low = quantile(values, probs = c(0.025))))
}
meian_bootstrap(data$grp1, data$grp2)



```

### Parametric bootstrap

* If the real-world distribution is known, we can just simulate the drawing of samples from such a distribution
* E.g. knowing population is normally distributed with given $\mu$ and $\sigma$ allows us to sample from that distribution (More realistic scenario: Overall outcome is a combination of events with known probabilities)
* Simulated sample is compared to actually measured sample
* Repeat many times

### Generating

# Data cleaning by `tidyverse`: `tidyr` \& `dplyr`

## Pipes in R: `%>%`

Rather than forcing the user to either save intermediate objects or nest functions, dplyr provides the %>% operator from magrittr. `x %>% f(y)` turns into `f(x, y)` so the result from one step is then “piped” into the next step.

## Create tidy data by `tidyr`
`tidyr` provides tools to help to create tidy data, where each column is a variable, each row is an observation, and each cell contains a single value. `tidyr` contains tools for changing the shape (pivoting) and hierarchy (nesting and 'unnesting') of a dataset, turning deeply nested lists into rectangular data frames ('rectangling'), and extracting values out of string columns. It also includes tools for working with missing values (both implicit and explicit).

### Wide to long: `gather()`

```{r gather}
rm(list = ls())
stocks <- data.frame(
        time = as.Date('2009-01-01') + 0:9,
        X = rnorm(10, 0, 1),
        Y = rnorm(10, 0, 2),
        Z = rnorm(10, 0, 4)
)
head(stocks, n = 3)
stocksm <- stocks %>% gather("stock", "price", -time)
head(stocksm, n = 3)
```

### Long to wide: `spread()`

```{r spread}
stocksm %>% spread("stock", "price")
stocksm %>% spread("time", "price")
```

### `drop_na()`: referr2 NA removal

### TODO: `separate()` \& `extract()` \& `unite()`

## Manipulating rectangular data by `dplyr`

Please be aware that `dplyr` do not cooperate very will with NAs. Have them removed!

### Filtering data with certain rules by `dplyr::filter()`

`filter()` allows you to select a subset of rows in a data frame. Like all single verbs, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame, selecting rows where the expression is TRUE.

You have to use `dplyr::filter` due to the conflicts. We'll use `mpg` dataset from `ggplot2`

```{r filter}
rm(list = ls())
dim(mpg)
dim(dplyr::filter(mpg))
# Will get a dataframe same as mpg
dim(dplyr::filter(mpg, hwy >= 24))
# Which is equal to:
dim(mpg[mpg$hwy >= 24,])
# or:
dim(mpg %>% dplyr::filter(hwy >= 24))
```

### select rows by `slice()`

```{r slice}
slice(mpg, 1:3)
# Which is equal to:
mpg[1:3,]
slice_head(mpg, n = 3)
# Which is equal to:
head(mpg, n = 3)
# There is also slice_tail()
slice_sample(mpg, n = 3)
# Which is equal to:
mpg[sample(c(1:dim(mpg)[1]), 3),]
# slice_max(data, which column, n=?)
slice_max(mpg, hwy, n = 3)
# There is also slice_min()
```

### Select columns by `select()`

```{r select}
head(select(mpg, cty, hwy), n = 3)
# Use : to select range of columns
head(select(mpg, trans:hwy), n = 3)
# Use ! to exclude columns
head(select(mpg, !(trans:hwy)), n = 3)
# Use other helpers
## Use everything() to select all columns
head(select(mpg, everything()), n = 3)
## Use last_col(offset) to select the last `offset` column
head(select(mpg, last_col(offset = 1)), n = 3)
## Start with & End with & Contains
head(select(mpg, starts_with("m")), n = 3)
head(select(mpg, ends_with("y")), n = 3)
head(select(mpg, contains("y")), n = 3)
```

### Sort the records by `arrange()`

To sort the dataset by the column provded by arguments.

```{r arrange}
mpg1 <- slice_sample(mpg, n = 20)
mpg1
arrange(mpg1, year, hwy)
# In reversed order:
arrange(mpg1, desc(year), hwy)
# Which is equal to:
mpg1[order(-mpg1$year, mpg1$hwy),]
```

### Add new columns with `mutate()` \& `transmute()`

This function generates a new column based on old columns.

```{r mutate transmutate}
head(mutate(mpg1, diff = hwy - cty), n = 3)
# Which is equal to:
head(transform(mpg1, diff = hwy - cty), n = 3)
# Use transmute() to keep mutated values only.
head(transmute(mpg1, diff = hwy - cty), n = 3)
```

### Rename columns with `rename()`

```{r rename}
head(rename(mpg, make = manufacturer, mod = model), n = 3)
# Change the values when select by `select()`
head(select(mpg, make = manufacturer, mod = model), n = 3)
```

### Change column order with `relocate()`

Use a similar syntax as `select()` to move blocks of columns at once.

```{r relocate}
head(relocate(mpg, year:fl, .before = model), n = 3)
# You may also use `.after`
```

### Summarise values with `summarise()` FAILED

# Plotting by `ggplot2` \& `viridis` \& basic plotting functions

## Plotting Devices

In this example, we will use `plot(a)` as our plot, which should be:

```{r plot}
rm(list = ls())
a <- rnorm(5, mean = 100, sd = 7)
plot(a)
```

```{r plotting devices, eval = FALSE}
png(filename = "demo.png", bg = "white")
plot(a)
dev.off()
jpeg(filename = "demo.jpg")
plot(a)
dev.off()
bmp(filename = "demo.bmp")
plot(a)
dev.off()
tiff(filename = "demo.tiff")
plot(a)
dev.off()
postscript(file = "demo.ps")
plot(a)
dev.off()
pdf(file = "demo.pdf")
plot(a)
dev.off()
svg(filename = "demo.svg")
plot(a)
dev.off()
```

## Plotting functions in `base`

### Histogram: `hist()`

```{r hist}
rm(list = ls())
hist(mpg$hwy,
     col = "red",
     xlab = "highway miles per gallon",
     ylab = "Frequency of highway miles per gallon")
```

### Boxplot: `boxplot()`

```{r boxplot}
rm(list = ls())
boxplot(mpg$cty, mpg$hwy, col = c("red", "green"),
        names = c("city miles per gallon", "highway miles per gallon"))
boxplot(mpg$cty, mpg$hwy, notch = TRUE, horizontal = TRUE)
```

### Scatter plot \& line plot: `plot()`

```{r plot_b}
plot(c(1:5), c(11:15), type = "b")
```

### Mosaic Plot by `mosaicplot`

```{r mosiacplot}
mat_test <- matrix(data = c(84, 82, 82, 57, 34, 11), nrow = 2)
dimnames(mat_test) <- list(Group = c("A", "B"),
                           Status = c("Good", "Fair", "Bad"))
mosaicplot(mat_test)
```

## Plotting functions in `stats`

### Normal Q-Q plot by `qqnorm()`

Omitted, see above.

## TODO: Advanced plotting by `ggplot2`

### Graphics can be saved by `ggsave()`.

```{text}
ggsave(filename, plot = last_plot(), device = NULL,
path = getwd(), scale = 1, width = NA, height = NA,
units = c("in", "cm", "mm"), dpi = 300,...)

Args:
    filename <- referr2 png
    plot <- ggplot. The plot you want to save.
    device <- device used.
        Avail: "eps", "ps", "tex" (pictex),
            "pdf", "jpeg", "tiff", "png", "bmp", "svg",  "wmf"
    path <- The directory your picture will in.
    scale <- numeric. Multiplicative scaling factor.
    width <- referr2 png
    height <- referr2 png
    units <- referr2 png
    ... <- arguments passed to graphical devices.
```


# String modification by `stringr` \& basic string functions

## Getting substrings by `str_sub()`

```{r str_sub}
hw <- c("Hadley Wickham")
str_sub(hw, 1, 6)
str_sub(hw, end = 6)
str_sub(hw, 8, 14)
str_sub(hw, start = 8)
str_sub(hw, c(1, 8), c(6, 14))
hw <- c("Hadley Wickham", "Ammonium Bicarbonate")
str_sub(hw, 1, 6)
str_sub(hw, end = 6)
str_sub(hw, 8, 14)
str_sub(hw, start = 8)
str_sub(hw, c(1, 8), c(6, 14))
```

## Replace with regex by `str_replace()` \& `str_replace_all()` \& `gsub()`

```{r str_replace}
fruits <- c("one apple", "two pears", "three bananas")
str_replace(fruits, "([aeiou])", "\\1\\1")
str_replace(fruits, "[aeiou]", c("1", "2", "3"))
```

## Split strings by `str_split()`

```{r str_split}
fruits <- c(
        "apples and oranges and pears and bananas",
        "pineapples and mangos and guavas"
)

str_split(fruits, " and ")
str_split(fruits, " and ", simplify = TRUE)
```

## Join strings together by `paste()` or `paste0()`

Omitted

# Machine Learning

## Clustering using `factoextra` `cluster`

```{r cluster}
rm(list = ls())
# remove any missing value (i.e, NA values for not available)
USArrests <- na.omit(USArrests)
# Normalize large variations
df <- scale(USArrests)

## Compute the gap statistic
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25, K.max = 10, B = 500)
# Plot the result
fviz_gap_stat(gap_stat)
km.res <- kmeans(df, 4, nstart = 25)
# Visualize clusters using factoextra
fviz_cluster(km.res, USArrests)


# eclust can automatically perform satistical tests in best effect
res.km <- eclust(df, "kmeans")
res.hc <- eclust(df, "hclust")
fviz_cluster(res.hc) # scatter plot
```

## Neuron Network

### `keras` on `iris` Dataset

```{r keras}
summary(iris)
iris_rand <- iris[order(runif(dim(iris)[1])),]
iris_as <- list(train = list(
        x = as.matrix(iris_rand[1:100, c("Sepal.Length",
                                         "Sepal.Width",
                                         "Petal.Length",
                                         "Petal.Width")]),
        y = as.numeric(iris_rand[1:100, "Species"]) - 1),
                test = list(
                        x = as.matrix(
                                iris_rand[101:150,
                                          c("Sepal.Length",
                                            "Sepal.Width",
                                            "Petal.Length",
                                            "Petal.Width")]),
                        y = as.numeric(iris_rand[101:150,
                                                 "Species"]) - 1))
model <- keras_model_sequential() %>%
        # Flatten a given input, does not affect the batch size.
        layer_flatten(input_shape = c(1, 4)) %>%
        # Set hidden layers with relu activation function
        layer_dense(units = 128, activation = "relu") %>%
        layer_dense(units = 128, activation = "relu") %>%
        layer_dense(units = 128, activation = "relu") %>%
        layer_dense(units = 128, activation = "relu") %>%
        layer_dense(units = 128, activation = "relu") %>%
        layer_dense(units = 128, activation = "relu") %>%
        layer_dropout(0.2) %>% # Set momentumn
        layer_dense(3, activation = "softmax") # Set output layer
summary(model)
model %>%
        compile(
                loss = "sparse_categorical_crossentropy",
                optimizer = "adam",
                metrics = "accuracy"
        )
model %>%
        fit(
                x = iris_as$train$x, y = iris_as$train$y,
                epochs = 50,
                validation_split = 0.3,
                verbose = 2
        )
model %>%
        evaluate(iris_as$test$x, iris_as$test$y, verbose = 0)
```

### `nnet` on `iris` Dataset

```{r nnet}
iris_rand <- iris[order(runif(dim(iris)[1])),]
train <- (iris_rand[1:100,])
test <- (iris_rand[101:150,])

iris.nn <- nnet(Species ~ ., data = train,
                size = 5, rang = 0.1,
                decay = 5e-4, maxit = 200)
iris.predict <- predict(iris.nn, test, type = "class")
table(test$Species, iris.predict)
```

### `Torch` HelloWorld

```{r torch,eval=FALSE}
library(palmerpenguins)
penguins_dataset <- dataset(
        name = "penguins_dataset",
        initialize = function(df) {
          df <- na.omit(df)
          # continuous input data (x_cont)
          x_cont <- df[, c("bill_length_mm",
                           "bill_depth_mm",
                           "flipper_length_mm",
                           "body_mass_g",
                           "year")] %>%
                  as.matrix()
          self$x_cont <- torch_tensor(x_cont)
          # categorical input data (x_cat)
          x_cat <- df[, c("island", "sex")]
          x_cat$island <- as.integer(x_cat$island)
          x_cat$sex <- as.integer(x_cat$sex)
          self$x_cat <- as.matrix(x_cat) %>% torch_tensor()
          # target data (y)
          species <- as.integer(df$species)
          self$y <- torch_tensor(species)
        },
        .getitem = function(i) {
          list(x_cont = self$x_cont[i,],
               x_cat = self$x_cat[i,],
               y = self$y[i])
        },
        .length = function() {
          self$y$size()[[1]]
        }
)
train_indices <- sample(seq_len(nrow(penguins)), 250)
train_ds <- penguins_dataset(penguins[train_indices,])
valid_ds <- penguins_dataset(penguins[setdiff(seq_len(nrow(penguins)), train_indices),])
train_dl <- train_ds %>% dataloader(batch_size = 16, shuffle = TRUE)
valid_dl <- valid_ds %>% dataloader(batch_size = 16, shuffle = FALSE)
embedding_module <- nn_module(
        initialize = function(cardinalities) {
          self$embeddings <- nn_module_list(
                  lapply(cardinalities,
                         function(x) nn_embedding(
                                 num_embeddings = x,
                                 embedding_dim = ceiling(x / 2))))
        },
        forward = function(x) {
          embedded <- vector(mode = "list",
                             length = length(self$embeddings))
          for (i in seq_along(self$embeddings)) {
            embedded[[i]] <- self$embeddings[[i]](x[, i])
          }
          torch_cat(embedded, dim = 2)
        }
)
net <- nn_module(
        "penguin_net",
        initialize = function(cardinalities,
                              n_cont,
                              fc_dim,
                              output_dim) {
          self$embedder <- embedding_module(cardinalities)
          self$fc1 <- nn_linear(sum(purrr::map(
                  cardinalities, function(x) ceiling(x / 2))
                                            %>% unlist()) + n_cont, fc_dim)
          self$output <- nn_linear(fc_dim, output_dim)
        },

        forward = function(x_cont, x_cat) {
          embedded <- self$embedder(x_cat)
          all <- torch_cat(
                  list(embedded, x_cont$to(dtype = torch_float())), dim = 2)
          all %>%
                  self$fc1() %>%
                  nnf_relu() %>%
                  self$output() %>%
                  nnf_log_softmax(dim = 2)
        }
)
model <- net(
        cardinalities = c(length(levels(penguins$island)),
                          length(levels(penguins$sex))),
        n_cont = 5,
        fc_dim = 32,
        output_dim = 3
)
optimizer <- optim_adam(model$parameters, lr = 0.01)

for (epoch in 1:20) {
  model$train()
  train_losses <- c()
  coro::loop(for (b in train_dl) {
    optimizer$zero_grad()
    output <- model(b$x_cont, b$x_cat)
    loss <- nnf_nll_loss(output, b$y)
    loss$backward()
    optimizer$step()
    train_losses <- c(train_losses, loss$item())
  })
  model$eval()
  valid_losses <- c()
  coro::loop(for (b in valid_dl) {
    output <- model(b$x_cont, b$x_cat)
    loss <- nnf_nll_loss(output, b$y)
    valid_losses <- c(valid_losses, loss$item())
  })
  cat(sprintf("Loss at epoch %d: training: %3.3f, validation: %3.3f\n",
              epoch, mean(train_losses), mean(valid_losses)))
}
```

## Decision-Making Tree
### `C50` on `iris` Dataset

```{r C50}
iris_rand <- iris[order(runif(dim(iris)[1])),]
train <- (iris_rand[1:100,])
test <- (iris_rand[101:150,])
iris.c50 <- C5.0(
        train[, c("Petal.Width", "Petal.Length", "Sepal.Width", "Sepal.Length")],
        train$Species)
iris.predict <- predict(iris.c50, test, type = "class")
table(test$Species, iris.predict)
```

# What we Have Now?

```{r End}
sessionInfo()
```
